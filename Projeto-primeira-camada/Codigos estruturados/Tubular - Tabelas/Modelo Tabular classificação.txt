import tensorflow as tf
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models import Word2Vec
from transformers import BertTokenizer, TFBertModel

class TabularClassificationModule:
    def __init__(self, corpus, num_classes):
        self.corpus = corpus
        self.num_classes = num_classes
        self.tfidf_vectorizer = TfidfVectorizer()
        self.word2vec_model = Word2Vec(sentences=[sentence.split() for sentence in corpus], vector_size=100, window=5, min_count=1, sg=0)
        self.bert_layer = BERTLayer()
    
    def tfidf_representation(self):
        tfidf_results = self.tfidf_vectorizer.fit_transform(self.corpus)
        return tfidf_results
    
    def word2vec_representation(self):
        word2vec_results = [sum(self.word2vec_model.wv[word] for word in sentence.split()) / len(sentence.split()) for sentence in self.corpus]
        return tf.convert_to_tensor(word2vec_results)
    
    def bert_representation(self):
        bert_results = self.bert_layer.encode_text(self.corpus)
        return bert_results
    
    def build_combined_model(self):
        tfidf_results = self.tfidf_representation()
        word2vec_results = self.word2vec_representation()
        bert_results = self.bert_representation()
        
        combined_results = tf.concat([tfidf_results.toarray(), word2vec_results, bert_results], axis=-1)
        
        inputs = tf.keras.layers.Input(shape=(combined_results.shape[1],))
        x = tf.keras.layers.Dense(units=128, activation='relu')(inputs)
        x = tf.keras.layers.Dense(units=64, activation='relu')(x)
        output = tf.keras.layers.Dense(units=self.num_classes, activation='softmax')(x)
        
        model = tf.keras.models.Model(inputs=inputs, outputs=output)
        return model

class BERTLayer:
    def __init__(self):
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.model = TFBertModel.from_pretrained('bert-base-uncased')
        
    def encode_text(self, text_list):
        encoded_input = self.tokenizer(text_list, padding=True, truncation=True, return_tensors='tf')
        outputs = self.model(**encoded_input)
        embeddings = outputs.last_hidden_state
        mean_embeddings = tf.reduce_mean(embeddings, axis=1)
        return mean_embeddings
