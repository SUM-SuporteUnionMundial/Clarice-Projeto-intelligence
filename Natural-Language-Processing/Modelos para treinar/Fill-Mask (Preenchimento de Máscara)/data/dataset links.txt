- Fill-Mask (Preenchimento de Máscara): Para esta tarefa, você pode usar um dos seguintes conjuntos de dados:
    - [MLM]: Um conjunto de dados que consiste em textos mascarados gerados a partir do corpus OSCAR, um grande corpus multilíngue extraído da web. Este conjunto de dados é adequado para treinar modelos que possam prever palavras ou tokens mascarados em um texto usando o contexto.
    https://paperswithcode.com/dataset/mlm
    - [Wikiann]: Um conjunto de dados que consiste em textos mascarados gerados a partir do corpus Wikiann, um corpus anotado com entidades nomeadas em 282 idiomas. Este conjunto de dados é adequado para treinar modelos que possam prever entidades nomeadas mascaradas em um texto usando o contexto.
    https://huggingface.co/datasets/wikiann
    - [TyDi QA]: Um conjunto de dados que consiste em textos mascarados gerados a partir do corpus TyDi QA, um corpus para perguntas e respostas em 11 idiomas tipologicamente diversos. Este conjunto de dados é adequado para treinar modelos que possam prever palavras ou tokens mascarados relacionados a perguntas e respostas usando o contexto.
    https://github.com/google-research-datasets/tydiqa
    - [Lince]: Um conjunto de dados que consiste em textos mascarados gerados a partir do corpus Lince, um corpus anotado com etiquetas gramaticais em português brasileiro. Este conjunto de dados é adequado para treinar modelos que possam prever palavras ou tokens mascarados relacionados à estrutura morfológica usando o contexto.
    https://ritual.uh.edu/lince/datasets
